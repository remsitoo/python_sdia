{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 2 - Practise with classic libraries\n",
    "\n",
    "Students (pair):\n",
    "- [Rémi Offner](https://github.com/remsitoo)\n",
    "- [Téis Matencio](https://github.com/Teis-03)"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conda create --name=lab2 --file=requirement.txt\n",
    "conda activate lab2\n",
    "# do not forget to deactivate the environment if needed\n",
    "# you can remove the environment once you are done\n",
    "conda env remove --name=lab2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful references for this lab**:\n",
    "\n",
    "[1] `numpy`: [lecture notes (1.4.1-1.4.2)](https://scipy-lectures.org/intro/numpy/index.html) and [documentation](https://numpy.org/doc/stable/)\n",
    "\n",
    "[2] `pandas`: [documentation](https://pandas.pydata.org/docs/getting_started/index.html), [quick tutorial](https://pandas.pydata.org/pandas-docs/version/0.15/10min.html)\n",
    "\n",
    "[3] `matplotlib`: [lecture notes (1.5)](https://scipy-lectures.org/intro/matplotlib/index.html) and [documentation](https://matplotlib.org/)\n",
    "\n",
    "[4] `h5py`: [quick start guide](http://docs.h5py.org/en/stable/quick.html#quick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"content\">Contents</a>\n",
    "- [Exercise 1: Computing basic statistics](#ex1)\n",
    "- [Exercise 2: Random variables and histograms](#ex2)\n",
    "- [Exercise 3: Discrete isotropic total variation](#ex3)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"ex1\">Exercise 1: Random variables and histograms</a>\n",
    "\n",
    "In this exercise, we are interested in generating samples from the Gamma distribution $\\mathcal{G}(\\alpha,\\beta)$, of probability density function (pdf)\n",
    "\n",
    "\\begin{equation}\n",
    "    p(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x) \\mathbb{1}_{\\mathbb{R}_+^*}(x),\n",
    "\\end{equation}\n",
    "\n",
    "and displaying their histogram. In the following, we consider $(\\alpha, \\beta) = (9, 2)$.\n",
    "\n",
    "1\\. Set the random seed to a fixed value for reproducibility, and biefly check your instruction works as intended.\n",
    "> Hint: you may take a look at the following pages: [random module](https://numpy.org/doc/stable/reference/random/index.html?highlight=random#module-numpy.random), [random generator](https://numpy.org/doc/stable/reference/random/generator.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Generate $\\approx 10^5$ samples in a vector. Save the vector in a file, `samples.hdf5` or `samples.npy`.\n",
    "> Warning / hint: \n",
    "> - take a careful look at the [documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.gamma.html?highlight=gamma#numpy.random.gamma) (multiple conventions exist for the definition of the pdf underlying the distribution...);\n",
    "> - to save data in a `npy` file, take a look at the example reported in the [Numpy documentation](https://numpy.org/doc/stable/reference/generated/numpy.save.html);\n",
    "> - to save data in a `.h5` file, take a quick look at the [documentation here](https://docs.h5py.org/en/stable/quick.html#quick)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Estimate an histogram of this distribution for a well chosen set of bins, and display it.\n",
    "> Warnings: \n",
    "> - make sure the [histogram](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html?highlight=hist#matplotlib.pyplot.hist) corresponds to a probability density function (pdf);\n",
    "> - do not forget to include a proper title with names for the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Overlay the probability density function on the histogram and compare these in a few words. Save the resulting picture in `.png` format.\n",
    "> Hint: \n",
    "> - take a look at the `scipy` [documentation](https://docs.scipy.org/doc/scipy/reference/stats.html) to avoid implementing the pdf from scratch;\n",
    "> - return the bins in which the histogram is computed, and evaluate the pdf on those points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"ex2\">Exercise 2: Basic statistics with `pandas`</a>\n",
    "\n",
    "In this second exercise, we focus on computing basic statistics, and applying linear regression to a small data set. These data are gathered in the following table, which gives the infant mortality (`X`) and the gross national product per inhabitant (`Y`) of 12 european countries :\n",
    "\n",
    "| `X` | 190 | 128 | 180 | 212 | 56 | 192 | 68 | 98 | 110 | 197 | 181 | 233 |\n",
    "|-----|-----|-----|-----|----|-----|----|----|-----|-----|-----|-----|-----|\n",
    "| `Y` |  24 |  28 |  24 | 19 |  37 | 22 | 34 |  25 |  36 |  24 |  20 |  18 |\n",
    "\n",
    "1\\. For `X `and `Y`, compute the median, mean, variance and standard deviation. The data points have already been entered into a `.csv` file stored in `data/data.csv`.\n",
    "> Hint: \n",
    "> - you can directly use `pandas` to load the data into a `DataFrame` ([`pd.read_csv`](https://pandas.pydata.org/docs/reference/frame.html));\n",
    "> - take a look at the built-in operations available for `DataFrame` objects ([documentation](https://pandas.pydata.org/docs/reference/frame.html));\n",
    "> - to display a `DataFrame` `f`:\n",
    "> ```python \n",
    "> from IPython.display import display\n",
    "> display(df)\n",
    "> ```\n",
    "> - sort the `DataFrame` with respect to the value of `X` (see [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values)) This will be useful for question 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Give the equation of the regression line of `Y` as a function of `X`.\n",
    "> Hint: \n",
    "> - take a look at the functionalities available in `numpy` (e.g., `np.polyfit` and `np.polyval`);\n",
    "> - if needed, note that you can retrieve the data from the resulting `pandas` `DataFrame` with the `to_numpy()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Display the cloud of points and the regression line $Y = f(X)$ on the same figure. Save the figure in `.png` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"ex3\">Exercise 3: Discrete isotropic total variation</a>\n",
    "\n",
    "This exercise is devoted to the computation of the discrete isotropic total variation (TV) of an input matrix $\\mathbf{X} = [\\mathbf{x}_n]_{1 \\leq n \\leq N} \\in\\mathbb{C}^{M \\times N}$, which is particularly useful in Bayesian inference (e.g., for inverse problems) to promote piece-wise smooth solutions. The TV is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{TV}(\\mathbf{X}) = \\Vert D(\\mathbf{X}) \\Vert_{1,2} = \\sum_{m=1}^M \\sum_{n=1}^N \\sqrt{[\\mathbf{XD}_h]^2_{m,n} + [\\mathbf{D}_v\\mathbf{X}]^2_{m,n}},\n",
    "\\end{equation*}\n",
    "\n",
    "where $[\\mathbf{Z}]_{m,n}$ denotes the elements in position $(m,n)$ of the matrix $\\mathbf{Z}$,\n",
    "\n",
    "\\begin{align*}\n",
    "    D(X) &= (\\mathbf{XD}_h, \\mathbf{D}_v\\mathbf{X}) \\in \\mathbb{C}^{M\\times N} \\times \\mathbb{C}^{M\\times N} \\\\\n",
    "    %\n",
    "    \\mathbf{XD}_h &= [\\mathbf{x}_2-\\mathbf{x}_1, \\dotsc, \\mathbf{x}_N-\\mathbf{x}_{N-1}, \\mathbf{0}_M] \\in \\mathbb{C}^{M\\times N} \\\\\n",
    "    %\n",
    "    \\mathbf{D}_v\\mathbf{X} &= [\\tilde{\\mathbf{x}}_2^T-\\tilde{\\mathbf{x}}^T_1, \\dotsc, \\tilde{\\mathbf{x}}^T_M-\\tilde{\\mathbf{x}}^T_{M-1}, \\mathbf{0}_N]^T \\in \\mathbb{C}^{M\\times N},\n",
    "\\end{align*}\n",
    "\n",
    "$\\mathbf{x}_n \\in \\mathbb{C}^{M}$ is the $n$-th column of $\\mathbf{X}$, and $\\tilde{\\mathbf{x}}_m \\in \\mathbb{C}^{1\\times N}$ is the $m$-th row of $\\mathbf{X}$. \n",
    "The linear operator $D: \\mathbb{C}^{M\\times N} \\rightarrow \\mathbb{C}^{M\\times N} \\times \\mathbb{C}^{M\\times N} $ is the discrete gradient operator. The adjoint of $D$, $D^*: \\mathbb{C}^{M\\times N} \\times \\mathbb{C}^{M\\times N} \\rightarrow \\mathbb{C}^{M\\times N}$, is given by\n",
    "\n",
    "\\begin{align*}\n",
    "    (\\forall \\mathbf{Y} = (\\mathbf{Y}_h,\\mathbf{Y}_v)), \\quad D^*(\\mathbf{Y}) &= \\mathbf{Y}_h\\mathbf{D}^*_h + \\mathbf{D}^*_v\\mathbf{Y}_v \\\\\n",
    "    %\n",
    "    \\mathbf{Y}_h\\mathbf{D}^*_h &= \\big[-\\mathbf{y}_{h,1},- [\\mathbf{y}_{h,n}-\\mathbf{y}_{h,n-1}]_{2 \\leq n \\leq N-1}, \\mathbf{y}_{h, N-1} \\big] \\\\\n",
    "    %\n",
    "    \\mathbf{D}^*_v\\mathbf{Y}_v &= \\big[-\\tilde{\\mathbf{y}}_{v,1}^T,- [\\tilde{\\mathbf{y}}_{v,m}^T-\\tilde{\\mathbf{y}}^T_{v,m-1}]_{2 \\leq m \\leq M-1}, \\tilde{\\mathbf{y}}^T_{v, M-1} \\big]^T\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{y}_{h,n}$ is the $n$-th column of $\\mathbf{Y}_h$, and $\\tilde{\\mathbf{x}}_{v,m}$ is the $m$-th row of $\\mathbf{Y}_v$.\n",
    "\n",
    "1\\. Using `numpy`, implement a function `gradient2D` to compute the 2D discrete gradient operator $D$ applied to a matrix $\\mathbf{X}\\in\\mathbb{C}^{M \\times N}$ (no for loops!). Trigger an error message whenever the input array has more than 2 dimensions. If not clear from the implementation, add a few short comments to explain your code.\n",
    "\n",
    "> Hint: \n",
    "> - to trigger an error, you can for instance use an `assert` statement, or raise an [exception (e.g., `AssertionError`)](https://docs.python.org/3/library/exceptions.html);\n",
    "> - only a few operations are needed: computing vertical differences, horizontal differences, and possibly a concatenation of matrices into a single tensor (= n-dimensional array);\n",
    "> - possibly useful functions: `np.diff`, `np.c_`, `np.r_` (or `np.concatenate`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gradient2D(X: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    # Dimension check\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"Input must be a 2D matrix\")\n",
    "\n",
    "    M, N = X.shape\n",
    "\n",
    "    # Horizontal differences (diff along axis=1 = columns)\n",
    "    XDh = np.concatenate((np.diff(X, axis=1), np.zeros((M, 1), dtype=X.dtype)), axis=1)\n",
    "\n",
    "    # Vertical differences (diff along axis=0 = rows)\n",
    "    DvX = np.concatenate((np.diff(X, axis=0), np.zeros((1, N), dtype=X.dtype)), axis=0)\n",
    "\n",
    "    return XDh, DvX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Implement a unit-test to validate the behaviour of the `gradient2D` function. For instance, you can check the format of the output, and test the result when the function is evaluated on a constant matrix (for both a square and a non-square input matrix). Run the unit-test from the present Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -q\n",
    "import pytest\n",
    "\n",
    "def test_gradient2D_output_shape():\n",
    "    \"\"\"Check that the function returns two arrays of the same shape as input.\"\"\"\n",
    "    X = np.random.randn(4, 5)\n",
    "    XDh, DvX = gradient2D(X)\n",
    "\n",
    "    assert XDh.shape == X.shape\n",
    "    assert DvX.shape == X.shape\n",
    "\n",
    "\n",
    "def test_gradient2D_constant_matrix_square():\n",
    "    \"\"\"Gradient of a constant square matrix should be zero everywhere.\"\"\"\n",
    "    X = np.ones((3, 3))\n",
    "    XDh, DvX = gradient2D(X)\n",
    "\n",
    "    assert np.allclose(XDh, 0)\n",
    "    assert np.allclose(DvX, 0)\n",
    "\n",
    "\n",
    "def test_gradient2D_constant_matrix_rectangular():\n",
    "    \"\"\"Gradient of a constant non-square matrix should also be zero everywhere.\"\"\"\n",
    "    X = np.full((2, 4), 7.5)  # constant 2x4\n",
    "    XDh, DvX = gradient2D(X)\n",
    "\n",
    "    assert np.allclose(XDh, 0)\n",
    "    assert np.allclose(DvX, 0)\n",
    "\n",
    "\n",
    "def test_gradient2D_invalid_dimension():\n",
    "    \"\"\"Input must be 2D — higher dimensional input should raise ValueError.\"\"\"\n",
    "    X = np.ones((2, 2, 2))\n",
    "    with pytest.raises(ValueError):\n",
    "        gradient2D(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Document the function `gradient2D` with an appropriate docstring (see Lab 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "gradient2D.__doc__ = \"\"\"\n",
    "    Compute the 2D discrete gradient operator D(X).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input 2D array (matrix).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (XDh, DvX) : tuple of np.ndarray\n",
    "        - XDh: horizontal differences (M x N)\n",
    "        - DvX: vertical differences (M x N)\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input matrix is not in 2 dimensions.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Using 1., define a function `tv` to compute $\\text{TV}(\\mathbf{X})$, $\\mathbf{X}\\in\\mathbb{C}^{M \\times N}$. Write a unit-test and document your function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "def tv(X: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the discrete isotropic Total Variation (TV) of a 2D matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        2D array (real or complex).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The total variation of X.\n",
    "    \"\"\"\n",
    "    # Get horizontal and vertical gradients\n",
    "    XDh, DvX = gradient2D(X)\n",
    "\n",
    "    # Compute sqrt(|XDh|^2 + |DvX|^2) elementwise, then sum\n",
    "    return np.sum(np.sqrt(np.abs(XDh)**2 + np.abs(DvX)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -q\n",
    "\n",
    "def test_tv_constant_matrix_square():\n",
    "    \"\"\"TV of a constant square matrix should be zero.\"\"\"\n",
    "    X = np.ones((3, 3))\n",
    "    assert tv(X) == 0\n",
    "\n",
    "\n",
    "def test_tv_constant_matrix_rectangular():\n",
    "    \"\"\"TV of a constant rectangular matrix should be zero.\"\"\"\n",
    "    X = np.full((2, 4), 5)\n",
    "    assert tv(X) == 0\n",
    "\n",
    "\n",
    "def test_tv_simple_case():\n",
    "    \"\"\"Check TV for a simple 2x2 case with known differences.\"\"\"\n",
    "    # Matrix:\n",
    "    # [[0, 1],\n",
    "    #  [0, 0]]\n",
    "    X = np.array([[0, 1],\n",
    "                  [0, 0]], dtype=float)\n",
    "\n",
    "    # Horizontal differences should be: [[1, 0], [0, 0]]\n",
    "    # Vertical differences should be:   [[0, 1], [0, 0]]\n",
    "    # Elementwise sqrt( diff_h^2 + diff_v^2 ) =\n",
    "    # [[1, 1], [0, 0]]  => sum = 2\n",
    "    assert np.isclose(tv(X), 2.0)\n",
    "\n",
    "\n",
    "def test_tv_complex_case():\n",
    "    \"\"\"TV should work on complex matrices.\"\"\"\n",
    "    X = np.array([[1+1j, 2+2j],\n",
    "                  [3+3j, 4+4j]])\n",
    "    result = tv(X)\n",
    "    assert isinstance(result, (int, float, np.floating))\n",
    "    assert result >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Implement a function `gradient2D_adjoint` to compute $D^*(\\mathbf{Y})$, the adjoint of the 2D discrete gradient operator $D$ applied to $\\mathbf{Y}\\in\\mathbb{C}^{M \\times N}\\times \\mathbb{C}^{M \\times N}$. Add a few short comments to explain your code whenever appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([[1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [5, 5, 5, 5, 5], [8, 8, 8, 8, 8]])\n",
    "np.diff(Y)\n",
    "#Y[1:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "def gradient2D_adjoint(Y: tuple[np.ndarray, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the adjoint of the 2D discrete gradient operator D^*(Y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : tuple of (Yh, Yv)\n",
    "        - Yh: horizontal component, shape (M, N)\n",
    "        - Yv: vertical component, shape (M, N)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Matrix of shape (M, N), the result of D^*(Y).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the input tuple elements are not 2D matrices.\n",
    "    ValueError\n",
    "        If the input tuple elements don't have the same shape.\n",
    "    \"\"\"\n",
    "    Yh, Yv = Y\n",
    "\n",
    "    if Yh.ndim != 2 or Yv.ndim != 2:\n",
    "        raise ValueError(\"Both Y[0] and Y[1] must be 2D matrices of the same shape\")\n",
    "    if Yh.shape != Yv.shape:\n",
    "        raise ValueError(\"Yh and Yv must have the same shape\")\n",
    "\n",
    "    M, N = Yh.shape\n",
    "\n",
    "    # --- Horizontal adjoint YhDh* ---\n",
    "    # Leftmost column:\n",
    "    left = -Yh[:, [0]]\n",
    "    # Middle columns:\n",
    "    middle = -np.diff(Yh, axis=1)[:, :-1]\n",
    "    # Rightmost column:\n",
    "    right = Yh[:, [N-2]]\n",
    "\n",
    "    Yh_adj = np.concatenate((left, middle, right), axis=1)\n",
    "\n",
    "    # --- Vertical adjoint Dv*Yv ---\n",
    "    # Top row:\n",
    "    top = -Yv[[0], :]\n",
    "    # Middle rows:\n",
    "    middle = -np.diff(Yv, axis=0)[:-1, :]\n",
    "    # Bottom row:\n",
    "    bottom = Yv[[M-2], :]\n",
    "\n",
    "    Yv_adj = np.concatenate((top, middle, bottom), axis=0)\n",
    "\n",
    "    # Combine both contributions\n",
    "    return Yh_adj + Yv_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Implement a unit-test to validate `gradient2D_adjoint`, e.g., by checking the size of the output from the function and verifying that `gradient2D_adjoint` is adjoint to `gradient2D`, i.e., for any $\\mathbf{X}\\in\\mathbb{C}^{M \\times N}$ and $\\mathbf{Y}\\in\\mathbb{C}^{M \\times N}\\times \\mathbb{C}^{M \\times N}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\forall \\mathbf{X} \\in \\mathbb{C}^{M \\times N}, \\mathbf{Y} = (\\mathbf{Y}_h, \\mathbf{Y}_v) \\in \\mathbb{C}^{M \\times N} \\times \\mathbb{C}^{M \\times N}, \\;\n",
    "    %\n",
    "    \\langle D(\\mathbf{X}), \\mathbf{Y} \\rangle_{\\mathbb{C}^{M \\times N} \\times \\mathbb{C}^{M \\times N}} = \\langle \\mathbf{X}, D^*(\\mathbf{Y}) \\rangle_{\\mathbb{C}^{M \\times N}}, \n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align}\n",
    "    &\\forall \\mathbf{U}, \\mathbf{V} \\in \\mathbb{C}^{M \\times N}, \\; \\langle \\mathbf{U}, \\mathbf{V} \\rangle_{\\mathbb{C}^{M \\times N}} = \\text{Tr}(\\mathbf{U}^T \\mathbf{V}) = \\sum_{m=1}^M \\sum_{n=1}^N u_{m,n}^* v_{m,n}, \\\\\n",
    "    &\\forall \\mathbf{U} = (\\mathbf{U}_h, \\mathbf{U}_v), \\mathbf{V} = (\\mathbf{V}_h, \\mathbf{V}_v) \\in \\mathbb{C}^{M \\times N} \\times \\mathbb{C}^{M \\times N}, \\; \\langle \\mathbf{U}, \\mathbf{V} \\rangle_{\\mathbb{C}^{M \\times N} \\times \\mathbb{C}^{M \\times N}} = \\langle \\mathbf{U}_h, \\mathbf{V}_h \\rangle_{\\mathbb{C}^{M \\times N}} + \\langle \\mathbf{U}_v, \\mathbf{V}_v \\rangle_{\\mathbb{C}^{M \\times N}}.\n",
    "\\end{align}\n",
    "\n",
    "> Hint: to verify `gradient2D_adjoint` is the adjoint of `gradient2D`, evaluate the scalar products above for randomly drawn matrices. Set the random generator to a known state for reproducibility (see [Exercise 1](#ex1)).",
"\n",   
"> `np.conj` is useful."
]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_product_matrix(U: np.ndarray, V: np.ndarray) -> np.bool:\n",
    "    \"\"\"Inner product for C^{M x N}: sum of conjugate(U) * V.\"\"\"\n",
    "    return np.vdot(U, V)   # same as sum(conj(U)*V)\n",
    "\n",
    "\n",
    "def inner_product_pair(U: tuple[np.ndarray, np.ndarray],\n",
    "                       V: tuple[np.ndarray, np.ndarray]) -> np.bool:\n",
    "    \"\"\"Inner product for pairs: sum of inner products of components.\"\"\"\n",
    "    Uh, Uv = U\n",
    "    Vh, Vv = V\n",
    "    return inner_product_matrix(Uh, Vh) + inner_product_matrix(Uv, Vv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -q\n",
    "\n",
    "def test_gradient2D_adjoint_output_shape():\n",
    "    \"\"\"Check output shape matches input X.\"\"\"\n",
    "    X = np.random.randn(4, 5)\n",
    "    Yh = np.random.randn(4, 5)\n",
    "    Yv = np.random.randn(4, 5)\n",
    "    out = gradient2D_adjoint((Yh, Yv))\n",
    "    assert out.shape == X.shape\n",
    "\n",
    "\n",
    "def test_adjoint_property():\n",
    "    \"\"\"Verify <D(X), Y> = <X, D*(Y)> for random X, Y.\"\"\"\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "\n",
    "    M, N = 4, 5\n",
    "    X = rng.normal(size=(M, N)) + 1j * rng.normal(size=(M, N))\n",
    "    Yh = rng.normal(size=(M, N)) + 1j * rng.normal(size=(M, N))\n",
    "    Yv = rng.normal(size=(M, N)) + 1j * rng.normal(size=(M, N))\n",
    "    Y = (Yh, Yv)\n",
    "\n",
    "    # Left-hand side: <D(X), Y>\n",
    "    DX = gradient2D(X)\n",
    "    lhs = inner_product_pair(DX, Y)\n",
    "\n",
    "    # Right-hand side: <X, D*(Y)>\n",
    "    rhs = inner_product_matrix(X, gradient2D_adjoint(Y))\n",
    "\n",
    "    assert np.allclose(lhs, rhs, rtol=1e-12, atol=1e-12)\n",
    "\n",
    "def test_gradient2D_adjoint_with_values():\n",
    "    Yh = np.array([\n",
    "        [1, 2],\n",
    "        [3, 4]\n",
    "    ])\n",
    "    Yv = np.array([\n",
    "        [5, 6],\n",
    "        [7, 8]\n",
    "    ])\n",
    "    result = gradient2D_adjoint((Yh, Yv))\n",
    "    expected = np.array([\n",
    "        [-6, -5],\n",
    "        [2, 9]])\n",
    "    assert np.allclose(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bonus, **optional**]. Generalize the `gradient2D` to any number of dimensions ($\\mathbf{X} \\in \\mathbb{C}^{N_1 \\times N_2 \\times \\dotsc \\times N_p}$), i.e., by returning tensors obtained by computing differences along each of its dimensions.\n",
    "> Hint: \n",
    "> - you may use a loops here, and/or list comprehension. Using slice objects (see [np.s_](https://numpy.org/doc/stable/reference/generated/numpy.s_.html?highlight=s_#numpy.s_) and [this page](https://stackoverflow.com/questions/24432209/python-index-an-array-using-the-colon-operator-in-an-arbitrary-dimension)) can be an interesting option.\n",
    ">\n",
    "> - the definition of the scalar product above can be extended to the case of tensors as follows:\n",
    "\\begin{equation}\n",
    "    \\mathbf{U}, \\mathbf{V} \\in \\mathbb{C}^{N_1 \\times N_2 \\times \\dotsc \\times N_p}, \\; \\langle \\mathbf{U}, \\mathbf{V} \\rangle_{\\mathbb{C}^{N_1 \\times N_2 \\times \\dotsc \\times N_p}} =  \\sum_{n_1 = 1}^{N_1}  \\sum_{n_2 = 1}^{N_2} \\dotsc \\sum_{n_p = 1}^{N_p} u_{n_1, n_2, \\dotsc, n_p}^* v_{n_1, n_2, \\dotsc, n_p}   \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "def gradientND(X: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the discrete gradient operator D(X) for an N-dimensional array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input array of shape (N1, N2, ..., Np).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of np.ndarray\n",
    "        List of arrays, one per dimension, each of the same shape as X.\n",
    "        The i-th element contains the finite difference along axis i.\n",
    "    \"\"\"\n",
    "    if X.ndim < 1:\n",
    "        raise ValueError(\"Input must have at least 1 dimension\")\n",
    "\n",
    "    grads = []\n",
    "    # iteration on the number of dimensions\n",
    "    for axis in range(X.ndim):\n",
    "        # Here we take the forward differences along the current axis\n",
    "        diff = np.diff(X, axis=axis)\n",
    "\n",
    "        # We define the padding to append to the differenced array\n",
    "        pad_shape = list(X.shape)\n",
    "        pad_shape[axis] = 1\n",
    "        pad = np.zeros(pad_shape, dtype=X.dtype)\n",
    "\n",
    "        # Concatenate: differences + zero padding\n",
    "        g = np.concatenate((diff, pad), axis=axis)\n",
    "        grads.append(g)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "```bibtex\n",
    "@article{condat:hal-01309685,\n",
    "  TITLE = {{Discrete Total Variation: New Definition and Minimization}},\n",
    "  AUTHOR = {Condat, Laurent},\n",
    "  URL = {https://hal.archives-ouvertes.fr/hal-01309685},\n",
    "  JOURNAL = {{SIAM Journal on Imaging Sciences}},\n",
    "  PUBLISHER = {{Society for Industrial and Applied Mathematics}},\n",
    "  VOLUME = {10},\n",
    "  NUMBER = {3},\n",
    "  PAGES = {1258--1290},\n",
    "  YEAR = {2017},\n",
    "  MONTH = Aug,\n",
    "  DOI = {10.1137/16M1075247},\n",
    "  KEYWORDS = { variational image processing ; total variation ;  finite-difference schemes ;  coarea formula},\n",
    "  PDF = {https://hal.archives-ouvertes.fr/hal-01309685v3/file/Condat-newTV.pdf},\n",
    "  HAL_ID = {hal-01309685},\n",
    "  HAL_VERSION = {v3},\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-g3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
